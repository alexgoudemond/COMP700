{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Segmentation\n",
    "\n",
    "This notebook will explore the next set of segmenation options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Alexander Goudemond, Student Number: 219030365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, walk, mkdir, stat, remove\n",
    "from os import sep # used later on, in a function, to print directory contents\n",
    "from os.path import exists, basename, join\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "from PIL.Image import fromarray\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directories for the Processed of Data-Sets\n",
    "\n",
    "This section of the notebook will find a way to create directories for the images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file order of the data-set is important as we have manually segmented and manually tracked pictures, which we do not plan on processing. We need to find a way to generate the 2 processed data-sets without altering this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial option to consider, is generating a list of all the file paths to our images..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite simple, thankfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(startPath):\n",
    "    location_array = []\n",
    "    acceptable_folders = [\"\\\\01\", \"\\\\02\", \"SEG\", \"TRA\"]\n",
    "\n",
    "    for root, dirs, files in walk(startPath):\n",
    "        # skip this folder\n",
    "        if (\"OriginalZipped\" in root):\n",
    "            continue\n",
    "\n",
    "        elif (root[ -3 : ] not in acceptable_folders):\n",
    "            continue\n",
    "\n",
    "        location_array.append(root)\n",
    "    \n",
    "    return location_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = getcwd()\n",
    "desired_directory = \"..\\\\..\\\\Comp700_Processed_DataSets_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (current_directory + \"\\\\\" + desired_directory)\n",
    "location_array = get_directories(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']\n",
      "Number of folders: 96\n"
     ]
    }
   ],
   "source": [
    "# first 10\n",
    "print( location_array[0:10] ) \n",
    "print(\"Number of folders:\", len( location_array ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can use that variable to generate the locations for our processed images! We just need to replace the keyword \"Comp700_DataSets\" with our desired folder name, and everything else will follow nicely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further improve the folder readability though, by only keeping the Comp700_DataSets etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_string_array(position, array):\n",
    "    new_array = []\n",
    "\n",
    "    for item in array:\n",
    "        new_array.append( item[position : ])\n",
    "    \n",
    "    return new_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = len(current_directory + \"\\\\..\\\\..\\\\\")\n",
    "# print(position)\n",
    "\n",
    "reduced_location_array = cut_string_array(position, location_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']\n",
      "\n",
      "Number of folders: 96\n"
     ]
    }
   ],
   "source": [
    "# first 10\n",
    "print(reduced_location_array[ 0 : 10])\n",
    "print()\n",
    "print(\"Number of folders:\", len( reduced_location_array ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the keyword now to our destination folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_part_of_array(key_word, new_word, array):\n",
    "    new_array = []\n",
    "    temp = \"\"\n",
    "\n",
    "    for item in array:\n",
    "        temp = item.replace(key_word, new_word)\n",
    "        new_array.append(temp)\n",
    "    \n",
    "    return new_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_locations = replace_part_of_array(\"Comp700_Processed_DataSets_1\", \"Comp700_Segmented\", reduced_location_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01', 'Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']\n",
      "Number of folders: 96\n"
     ]
    }
   ],
   "source": [
    "print( desired_locations[0:10] )\n",
    "print(\"Number of folders:\", len( desired_locations ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! We now have a variable containing the folder locations! We can now define some functions to validate all directories exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for work we create\n",
    "def tryMakeDirectory(current_directory, destination_directory):\n",
    "    try:\n",
    "        # join comes from os.path\n",
    "        mkdir( join(current_directory, destination_directory) )\n",
    "    except FileExistsError:\n",
    "        # print(\"Folder already exists!\")\n",
    "        pass\n",
    "    except:\n",
    "        print(\"Unknown Error Encountered...\")\n",
    "###\n",
    "\n",
    "def createBulkDirectories(current_directory, array):\n",
    "    sub_folders = []\n",
    "    path = \"..\\\\..\\\\\"\n",
    "\n",
    "    for item in array:\n",
    "        sub_folders = item.split(\"\\\\\")\n",
    "        # print(sub_folders)\n",
    "\n",
    "        for folder in sub_folders:\n",
    "            path += folder\n",
    "            tryMakeDirectory(current_directory, path)\n",
    "            path += \"\\\\\"\n",
    "        \n",
    "        # reset\n",
    "        path = \"..\\\\..\\\\\"\n",
    "\n",
    "    print(\"Done!\")\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "createBulkDirectories(current_directory, desired_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Set Segmentation\n",
    "\n",
    "This section of the notebook focusses on processing the entire data-set, following the methods found in 005 for 1103_10 and 1103_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to take advantage of the Thresholding found with OpenCV - specifically the mask value of 17. Let's create a function to do that processing for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opencvThresh(img, value=17):\n",
    "    ret, thresh = cv2.threshold(img, 0, 255, value)\n",
    "\n",
    "    return thresh\n",
    "###\n",
    "\n",
    "# used to make the segmented values visible, by saving via matplotlib\n",
    "def getImage(filePath):\n",
    "    img = plt.imread(filePath) \n",
    "    plt.imsave(\"temp.jpg\", img, cmap=\"gray\") # desired colourmap for us\n",
    "    img = cv2.imread( \"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    return img\n",
    "###\n",
    "\n",
    "# process choice influences processOne or processTwo\n",
    "def bulkProcess(current_directory, original_dataset, location_array):\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    counter = 0\n",
    "    valid_folders = [\"01\", \"02\", \"SEG\", \"TRA\"]\n",
    "\n",
    "    name = \"segmented_\"\n",
    "\n",
    "    # go to the original_dataset\n",
    "    path = walk(current_directory + \"\\\\\" + original_dataset)\n",
    "    \n",
    "    print(\"Starting...\")\n",
    "\n",
    "    for root, dirs, files in path:\n",
    "        # skip zipped files\n",
    "        if (\"OriginalZipped\" in root):\n",
    "            continue\n",
    "        # end loop because locations exhausted\n",
    "        elif (counter >= len(location_array)):\n",
    "            break\n",
    "\n",
    "        # print(root)\n",
    "\n",
    "        for item in files:\n",
    "            # manual info, simply copy as is\n",
    "            if (\"man_\" in item):\n",
    "                # print(\"Counter:\", counter)\n",
    "                img_path = current_directory + \"\\\\..\\\\..\\\\\" + location_array[counter] + \"\\\\\" +  item\n",
    "                # print(img_path)\n",
    "\n",
    "                # handle text files\n",
    "                if (\".txt\" in item):\n",
    "                    copyfile(root + \"\\\\\" + item, img_path)\n",
    "                else:\n",
    "                    # print(\"EISH\")\n",
    "                    img = getImage(root + \"\\\\\" + item)\n",
    "                    cv2.imwrite(img_path, img)\n",
    "\n",
    "                \n",
    "            # stop working, zipped files found\n",
    "            elif (\".zip\" in item):\n",
    "                break\n",
    "            else:\n",
    "                # print(\"Nope\")\n",
    "\n",
    "                img = getImage(root + \"\\\\\" + item)\n",
    "\n",
    "                processed_pic = opencvThresh(img)\n",
    "\n",
    "                # print(\"Counter:\", counter)\n",
    "                img_path = current_directory + \"\\\\..\\\\..\\\\\" + location_array[counter] + \"\\\\\" + name + item\n",
    "                # print(img_path)\n",
    "\n",
    "                cv2.imwrite(img_path, processed_pic)\n",
    "\n",
    "            # remove later\n",
    "            # break\n",
    "        \n",
    "        # update counter\n",
    "        if (basename(root) in valid_folders):\n",
    "            counter += 1\n",
    "    \n",
    "    # remove at end\n",
    "    if (exists(\"temp.jpg\")):\n",
    "        remove(\"temp.jpg\")\n",
    "    \n",
    "    print(\"Finished...\")\n",
    "### \n",
    "\n",
    "def getFileQuantities(path):\n",
    "    count = 0\n",
    "    size_array = []\n",
    "    valid_folders = [\"01\", \"02\", \"SEG\", \"TRA\"]\n",
    "\n",
    "    for root, dirs, files in walk(path):\n",
    "        count = 0\n",
    "\n",
    "        for file in files:\n",
    "            count += 1\n",
    "        \n",
    "        if (basename(root) in valid_folders):\n",
    "            size_array.append(count)\n",
    "    \n",
    "    return size_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sizes = getFileQuantities( current_directory + \"\\\\\" + \"..\\\\..\\\\Comp700_DataSets\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1764,\n",
       " 49,\n",
       " 1765,\n",
       " 1764,\n",
       " 1764,\n",
       " 8,\n",
       " 1765,\n",
       " 1764,\n",
       " 1763,\n",
       " 1763,\n",
       " 1375,\n",
       " 50,\n",
       " 1377,\n",
       " 1376,\n",
       " 1376,\n",
       " 50,\n",
       " 1377,\n",
       " 1376,\n",
       " 1376,\n",
       " 1375,\n",
       " 84,\n",
       " 9,\n",
       " 85,\n",
       " 84,\n",
       " 84,\n",
       " 9,\n",
       " 85,\n",
       " 84,\n",
       " 115,\n",
       " 115,\n",
       " 30,\n",
       " 8,\n",
       " 31,\n",
       " 30,\n",
       " 5,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 48,\n",
       " 18,\n",
       " 49,\n",
       " 48,\n",
       " 48,\n",
       " 33,\n",
       " 49,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 92,\n",
       " 30,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 20,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 150,\n",
       " 150,\n",
       " 151,\n",
       " 110,\n",
       " 138,\n",
       " 92,\n",
       " 28,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 8,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 115,\n",
       " 15,\n",
       " 116,\n",
       " 115,\n",
       " 115,\n",
       " 19,\n",
       " 116,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 300,\n",
       " 2,\n",
       " 301,\n",
       " 300,\n",
       " 300,\n",
       " 2,\n",
       " 301,\n",
       " 300,\n",
       " 300,\n",
       " 300]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1764,\n",
       " 49,\n",
       " 1765,\n",
       " 1764,\n",
       " 1764,\n",
       " 8,\n",
       " 1765,\n",
       " 1764,\n",
       " 1763,\n",
       " 1763,\n",
       " 1375,\n",
       " 50,\n",
       " 1377,\n",
       " 1376,\n",
       " 1376,\n",
       " 50,\n",
       " 1377,\n",
       " 1376,\n",
       " 1376,\n",
       " 1375,\n",
       " 84,\n",
       " 9,\n",
       " 85,\n",
       " 84,\n",
       " 84,\n",
       " 9,\n",
       " 85,\n",
       " 84,\n",
       " 115,\n",
       " 115,\n",
       " 30,\n",
       " 8,\n",
       " 31,\n",
       " 30,\n",
       " 5,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 48,\n",
       " 18,\n",
       " 49,\n",
       " 48,\n",
       " 48,\n",
       " 33,\n",
       " 49,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 92,\n",
       " 30,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 20,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 150,\n",
       " 150,\n",
       " 151,\n",
       " 110,\n",
       " 138,\n",
       " 92,\n",
       " 28,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 8,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 115,\n",
       " 15,\n",
       " 116,\n",
       " 115,\n",
       " 115,\n",
       " 19,\n",
       " 116,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 300,\n",
       " 2,\n",
       " 301,\n",
       " 300,\n",
       " 300,\n",
       " 2,\n",
       " 301,\n",
       " 300,\n",
       " 300,\n",
       " 300]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_sizes = getFileQuantities( current_directory + \"\\\\\" + \"..\\\\..\\\\Comp700_Segmented\" )\n",
    "\n",
    "segmented_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if (original_sizes == segmented_sizes):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")\n",
    "    print(\"\\nGenerating now\")\n",
    "    bulkProcess(current_directory, \"..\\\\..\\\\Comp700_Processed_DataSets_1\", desired_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some folders provided by the data-sets that contain manually segmented images. Let us go through the segmented folder now and identify the quantities of images. We can then generate some videos and stitch them side by side, to identify the success of the segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We can take advantage of reduced_location_array to map segmented images onto the corresponding folder\n",
    "'''\n",
    "\n",
    "reduced_location_array[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_location_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "manSegPicArray = [0 for i in range(len(reduced_location_array))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manSegPicArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We can take advantage of reduced_location_array to map segmented images onto the corresponding folder,\n",
    "inside manSegPicArray\n",
    "'''\n",
    "\n",
    "desired_pics = \"COMP700_Segmented\"\n",
    "\n",
    "path = walk(current_directory + \"\\\\..\\\\..\\\\\" + desired_pics)\n",
    "    \n",
    "keyword = \"man_seg\"\n",
    "count = 0\n",
    "\n",
    "for root, dirs, files in path:\n",
    "    # print(files)\n",
    "    # print(count, end=\"; \")\n",
    "    for item in files:\n",
    "        if (keyword in item):\n",
    "            # print(len(files))\n",
    "            manSegPicArray[count] = len(files)\n",
    "            break\n",
    "        break\n",
    "\n",
    "    # only update if non empty set!\n",
    "    if ( len(files) != 0):\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 49,\n",
       " 0,\n",
       " 1764,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 1764,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 50,\n",
       " 0,\n",
       " 1376,\n",
       " 0,\n",
       " 50,\n",
       " 0,\n",
       " 1376,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 84,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 84,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 48,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 48,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 30,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 65,\n",
       " 0,\n",
       " 0,\n",
       " 150,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 28,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 115,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 115,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 300,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 300,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manSegPicArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's marry this information together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_GT\\SEG ... 49\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_ST\\SEG ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_GT\\SEG ... 8\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_ST\\SEG ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_GT\\SEG ... 50\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_ST\\SEG ... 1376\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_GT\\SEG ... 50\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_ST\\SEG ... 1376\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_GT\\SEG ... 9\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_GT\\SEG ... 9\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG ... 84\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\01_GT\\SEG ... 8\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\02_GT\\SEG ... 5\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_GT\\SEG ... 18\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_ST\\SEG ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_GT\\SEG ... 33\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_ST\\SEG ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_GT\\SEG ... 30\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_GT\\SEG ... 20\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\01_GT\\SEG ... 65\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\02_GT\\SEG ... 150\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_GT\\SEG ... 28\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_GT\\SEG ... 8\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_GT\\SEG ... 15\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_ST\\SEG ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_GT\\SEG ... 19\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_ST\\SEG ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_GT\\SEG ... 2\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_ST\\SEG ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_GT\\SEG ... 2\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_ST\\SEG ... 300\n",
      "\n",
      "There are 36 folders of segmented images\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(manSegPicArray)):\n",
    "    if (manSegPicArray[i] != 0):\n",
    "        print(reduced_location_array[i], manSegPicArray[i], sep=\" ... \")\n",
    "        count += 1\n",
    "\n",
    "print(\"\\nThere are\", count, \"folders of segmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extend that summary to see how many images are in the corresponding folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondingPicArray = [0 for i in range(len(reduced_location_array))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_pics = \"COMP700_Segmented\"\n",
    "\n",
    "path = walk(current_directory + \"\\\\..\\\\..\\\\\" + desired_pics)\n",
    "    \n",
    "keyword1 = \"_GT\"; keyword2 = \"_ST\"\n",
    "count = 0\n",
    "index = 0\n",
    "\n",
    "for root, dirs, files in path:\n",
    "    # print(files)\n",
    "    # print(count, end=\"; \")\n",
    "    for item in files:\n",
    "        if (keyword1 in reduced_location_array[index]) or (keyword2 in reduced_location_array[index]):\n",
    "            # print(len(files))\n",
    "            # print(count)\n",
    "            correspondingPicArray[index] = count\n",
    "            break\n",
    "        else:\n",
    "            count = len(files)\n",
    "            break\n",
    "    \n",
    "    if (len(files) != 0):\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1764,\n",
       " 1764,\n",
       " 1764,\n",
       " 0,\n",
       " 1764,\n",
       " 1764,\n",
       " 1764,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1375,\n",
       " 1375,\n",
       " 1375,\n",
       " 0,\n",
       " 1376,\n",
       " 1376,\n",
       " 1376,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 0,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 30,\n",
       " 30,\n",
       " 0,\n",
       " 30,\n",
       " 30,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 0,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 65,\n",
       " 65,\n",
       " 0,\n",
       " 150,\n",
       " 150,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 0,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 300,\n",
       " 300,\n",
       " 300,\n",
       " 0,\n",
       " 300,\n",
       " 300,\n",
       " 300,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correspondingPicArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_GT\\SEG ... Segmented Pictures: ... 49 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_ST\\SEG ... Segmented Pictures: ... 1764 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_GT\\SEG ... Segmented Pictures: ... 8 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_ST\\SEG ... Segmented Pictures: ... 1764 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_GT\\SEG ... Segmented Pictures: ... 50 ... Corresponding Pictures: ... 1375\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_ST\\SEG ... Segmented Pictures: ... 1376 ... Corresponding Pictures: ... 1375\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_GT\\SEG ... Segmented Pictures: ... 50 ... Corresponding Pictures: ... 1376\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_ST\\SEG ... Segmented Pictures: ... 1376 ... Corresponding Pictures: ... 1376\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_GT\\SEG ... Segmented Pictures: ... 9 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG ... Segmented Pictures: ... 84 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_GT\\SEG ... Segmented Pictures: ... 9 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG ... Segmented Pictures: ... 84 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\01_GT\\SEG ... Segmented Pictures: ... 8 ... Corresponding Pictures: ... 30\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\02_GT\\SEG ... Segmented Pictures: ... 5 ... Corresponding Pictures: ... 30\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_GT\\SEG ... Segmented Pictures: ... 18 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_ST\\SEG ... Segmented Pictures: ... 48 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_GT\\SEG ... Segmented Pictures: ... 33 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_ST\\SEG ... Segmented Pictures: ... 48 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_GT\\SEG ... Segmented Pictures: ... 30 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_GT\\SEG ... Segmented Pictures: ... 20 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\01_GT\\SEG ... Segmented Pictures: ... 65 ... Corresponding Pictures: ... 65\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\02_GT\\SEG ... Segmented Pictures: ... 150 ... Corresponding Pictures: ... 150\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_GT\\SEG ... Segmented Pictures: ... 28 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_GT\\SEG ... Segmented Pictures: ... 8 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_GT\\SEG ... Segmented Pictures: ... 15 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_ST\\SEG ... Segmented Pictures: ... 115 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_GT\\SEG ... Segmented Pictures: ... 19 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_ST\\SEG ... Segmented Pictures: ... 115 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_GT\\SEG ... Segmented Pictures: ... 2 ... Corresponding Pictures: ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_ST\\SEG ... Segmented Pictures: ... 300 ... Corresponding Pictures: ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_GT\\SEG ... Segmented Pictures: ... 2 ... Corresponding Pictures: ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_ST\\SEG ... Segmented Pictures: ... 300 ... Corresponding Pictures: ... 300\n",
      "\n",
      "There are 36 folders of segmented images\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(manSegPicArray)):\n",
    "    if (manSegPicArray[i] != 0):\n",
    "        print(reduced_location_array[i], \"Segmented Pictures:\", manSegPicArray[i], \"Corresponding Pictures:\", correspondingPicArray[i], sep=\" ... \")\n",
    "        count += 1\n",
    "\n",
    "print(\"\\nThere are\", count, \"folders of segmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed look at those results indicates that the GT Segmentation always <= quantity of images in the corresponding folder. Perhaps GT stands for Generated Trace? ST, on the other hand, is always = quantity of images in the corresponding folder.\n",
    "\n",
    "FLUO-N2DL-SIM+ as well as Fluo-C2DL-Huh7 also contain 2 folders of segmentated images, all the rest have 4. Meaning 8 * 4 + 2 * 2 = 32 + 4 = 36\n",
    "\n",
    "We can use this information to generate 10 videos, each stitched together, and watch them to see the the segmentation differences. We can then decide if we need to further segment, or can move to post processing immediately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import move # moves and replaces files\n",
    "\n",
    "from moviepy.editor import clips_array, VideoFileClip\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need an array of the folder locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Array\n",
      "['BF-C2DL-HSC', 'BF-C2DL-HSC (1)', 'BF-C2DL-MuSC', 'BF-C2DL-MuSC (1)', 'DIC-C2DH-HeLa', 'DIC-C2DH-HeLa (1)', 'Fluo-C2DL-Huh7', 'Fluo-C2DL-Huh7 (1)', 'Fluo-C2DL-MSC', 'Fluo-C2DL-MSC (1)', 'Fluo-N2DH-GOWT1', 'Fluo-N2DH-GOWT1 (1)', 'Fluo-N2DH-SIM+', 'Fluo-N2DH-SIM+ (1)', 'Fluo-N2DL-HeLa', 'Fluo-N2DL-HeLa (1)', 'PhC-C2DH-U373', 'PhC-C2DH-U373 (1)', 'PhC-C2DL-PSC', 'PhC-C2DL-PSC (1)']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We only need to show every _OTHER_ folder, as each data-set has a \n",
    " training and challenge set. So out of 20 files, we need to show 10\n",
    "\n",
    "First things first, let us create an array of the directory locations\n",
    "'''\n",
    "\n",
    "data_sets = \"..\\\\..\\\\Comp700_DataSets\"\n",
    "current_directory = getcwd()\n",
    "\n",
    "path = walk(current_directory + \"\\\\\" + data_sets)\n",
    "\n",
    "directory_array = [] # contains the main folders\n",
    "\n",
    "i = 1\n",
    "for root, dirs, files in path:\n",
    "    if (i == 2):\n",
    "        directory_array = dirs\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"Directory Array\")\n",
    "print(directory_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02\\\\']\n",
      "\n",
      " 96\n"
     ]
    }
   ],
   "source": [
    "def replace_part_of_array(key_word, new_word, array):\n",
    "    new_array = []\n",
    "    temp = \"\"\n",
    "\n",
    "    for item in array:\n",
    "        temp = item.replace(key_word, new_word)\n",
    "        new_array.append(temp)\n",
    "    \n",
    "    return new_array\n",
    "###\n",
    "\n",
    "# First, generate a list of the locations for each folder of Petri Dish images\n",
    "data_sets = \"..\\\\..\\\\Comp700_Segmented\"\n",
    "path = walk(current_directory + \"\\\\\" + data_sets) # reset path\n",
    "\n",
    "location_array = reduced_location_array\n",
    "\n",
    "location_array = replace_part_of_array(\"Comp700_Processed_DataSets_1\", \"Comp700_Segmented\", location_array)\n",
    "\n",
    "for i in range(len(location_array)):\n",
    "    location_array[i] = current_directory + \"\\\\..\\\\..\\\\\" + location_array[i] + \"\\\\\"\n",
    "\n",
    "\n",
    "print(location_array[0:10])\n",
    "\n",
    "print(\"\\n\", len(location_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BF-C2DL-HSC_01', 'BF-C2DL-HSC_01_GT_SEG', 'BF-C2DL-HSC_01_GT_TRA', 'BF-C2DL-HSC_01_ST_SEG', 'BF-C2DL-HSC_02', 'BF-C2DL-HSC_02_GT_SEG', 'BF-C2DL-HSC_02_GT_TRA', 'BF-C2DL-HSC_02_ST_SEG', 'BF-C2DL-HSC (1)_01', 'BF-C2DL-HSC (1)_02']\n"
     ]
    }
   ],
   "source": [
    "# generate this for the videos!\n",
    "folderNameArray = []\n",
    "\n",
    "for i in range(len(location_array)):\n",
    "    temp = location_array[i].replace(current_directory + \"\\\\..\\\\..\\\\\" + \"Comp700_Segmented\\\\\", \"\")\n",
    "    \n",
    "\n",
    "    #remove leading folder name and trailing braces\n",
    "    position = temp.index(\"\\\\\")\n",
    "    temp = temp[ position + 1 : -1 ]\n",
    "\n",
    "    temp = temp.replace(\"\\\\\", \"_\")\n",
    "\n",
    "    folderNameArray.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "print(folderNameArray[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know from 003 that the images have consistent dimensions. So, we fetch the dimensions using the first image from each folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [832, 992], [832, 992], [832, 992], [832, 992], [782, 1200], [782, 1200], [782, 1200], [782, 1200], [832, 992], [782, 1200], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [690, 628], [690, 628], [690, 628], [773, 739], [773, 739], [773, 739], [718, 660], [790, 664], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720]]\n"
     ]
    }
   ],
   "source": [
    "image_size_array = []\n",
    "\n",
    "path = walk(current_directory + \"\\\\\" + data_sets) # reset path\n",
    "\n",
    "i = -1 # will grow from 0 to 39\n",
    "for root, dirs, files in path:\n",
    "    for item in files:\n",
    "        # print(item)\n",
    "        if (\".txt\" not in item) and (\".zip\" not in item):\n",
    "            i += 1\n",
    "\n",
    "            img = cv2.imread( (location_array[i] + item), cv2.IMREAD_GRAYSCALE)\n",
    "            (x, y) = img.shape\n",
    "\n",
    "            image_size_array.append([x, y])\n",
    "            break\n",
    "            \n",
    "\n",
    "print(image_size_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateVideos(current_directory, desired_folder, use_colour):\n",
    "    # only progress if files don't exist\n",
    "    makeVideos = False\n",
    "\n",
    "    if (exists(current_directory + \"\\\\\" + desired_folder)):\n",
    "        # Now, go to directory and verify all is there\n",
    "        path = walk(current_directory + \"\\\\\" + desired_folder)\n",
    "\n",
    "        count = 0\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                count += 1\n",
    "\n",
    "        if (count == len(location_array)):\n",
    "            print(\"All Videos exist already!\")\n",
    "        else:\n",
    "            print(\"Not all Videos exist\")\n",
    "            makeVideos = True\n",
    "    else:\n",
    "        makeVideos = True\n",
    "        \n",
    "    if (makeVideos):\n",
    "        path = walk(current_directory + \"\\\\\" + data_sets) # reset path\n",
    "\n",
    "        i = -1\n",
    "        output_video = cv2.VideoWriter()\n",
    "        frames_per_second = 10\n",
    "        picNum = 0\n",
    "        fileName = \"\"\n",
    "\n",
    "        # Generates Colour Videos\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                if (\".txt\" not in item) and (\".zip\" not in item):\n",
    "                    picNum += 1\n",
    "                    # update on first element only\n",
    "                    if  (picNum == 1):\n",
    "                        i += 1\n",
    "                        index = i // 2 # used for output video as 2 copies for each directory\n",
    "\n",
    "                        size = (image_size_array[i][1], image_size_array[i][0] ) # notice order\n",
    "                        fileName = \"video_segmented_\" + folderNameArray[i] + \".mp4\"\n",
    "                        \n",
    "                        output_video = cv2.VideoWriter(\n",
    "                            fileName, \n",
    "                            cv2.VideoWriter_fourcc(*'DIVX'), \n",
    "                            frames_per_second, \n",
    "                            size,\n",
    "                            isColor=use_colour # either True or False\n",
    "                        )\n",
    "\n",
    "                    img = plt.imread(location_array[i] + item) \n",
    "                    plt.imsave(\"temp.jpg\", img, cmap='gray')\n",
    "                    img = cv2.imread( \"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "                      \n",
    "                    output_video.write(img)\n",
    "\n",
    "            picNum = 0 # reset\n",
    "\n",
    "            if (len(fileName) != 0):\n",
    "                cv2.destroyAllWindows()\n",
    "                output_video.release()\n",
    "                print(\"Video finished for \", fileName, sep=\"\")\n",
    "    \n",
    "    # remove at end\n",
    "    if (exists(\"temp.jpg\")):\n",
    "        remove(\"temp.jpg\")\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os.path import join\n",
    "# from shutil import move # moves and replaces files\n",
    "\n",
    "def moveBulkVideos(current_directory, desired_folder):\n",
    "    # only progress if files don't exist\n",
    "    if (exists(current_directory + \"\\\\\" + desired_folder)):\n",
    "        print(\"Videos already exist!\")\n",
    "    else:\n",
    "        # local function\n",
    "        tryMakeDirectory(current_directory, desired_folder)\n",
    "\n",
    "        path = walk(current_directory)\n",
    "\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                if (\".mp4\" in item):\n",
    "                    new_destination = current_directory + \"\\\\\" + desired_folder\n",
    "                    move(join(current_directory, item), join(new_destination, item)) # should overwrite existing data\n",
    "\n",
    "        # Now, go to directory and verify all is there\n",
    "        path = walk(current_directory + \"\\\\\" + desired_folder)\n",
    "\n",
    "        count = 0\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                count += 1\n",
    "\n",
    "        if (count == len(location_array)):\n",
    "            print(\"All Videos Moved Successfully!\")\n",
    "        else:\n",
    "            print(\"Not all Videos Moves Successfully\")\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video finished for video_segmented_BF-C2DL-HSC_01.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_01.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC (1)_01.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC (1)_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC (1)_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-HSC (1)_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_01.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_01.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC (1)_01.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC (1)_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC (1)_02.mp4\n",
      "Video finished for video_segmented_BF-C2DL-MuSC (1)_02.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_01.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_01.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa (1)_01.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa (1)_02.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa (1)_02.mp4\n",
      "Video finished for video_segmented_DIC-C2DH-HeLa (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_01.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_01.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7 (1)_01.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7 (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7 (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-Huh7 (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_01.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_01.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC (1)_01.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-C2DL-MSC (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1 (1)_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1 (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1 (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-GOWT1 (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+ (1)_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+ (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+ (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DH-SIM+ (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa (1)_01.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa (1)_02.mp4\n",
      "Video finished for video_segmented_Fluo-N2DL-HeLa (1)_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_01.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_01.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373 (1)_01.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373 (1)_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373 (1)_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DH-U373 (1)_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_01.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_01.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_01_GT_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_01_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_01_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02_GT_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02_GT_TRA.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC_02_ST_SEG.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC (1)_01.mp4\n",
      "Video finished for video_segmented_PhC-C2DL-PSC (1)_02.mp4\n"
     ]
    }
   ],
   "source": [
    "generateVideos(current_directory, \"..\\\\..\\\\Comp700_VideosOfSegmentation\", use_colour=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos already exist!\n"
     ]
    }
   ],
   "source": [
    "moveBulkVideos(current_directory, \"..\\\\..\\\\Comp700_VideosOfSegmentation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tracking_cells')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5172ef57c372de0319e4db714a6087489ed069afeb66a66d936cee1d14e4331d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
