{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Segmentation\n",
    "\n",
    "This notebook will explore the next set of segmenation options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Alexander Goudemond, Student Number: 219030365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, walk, mkdir, stat, remove\n",
    "from os import sep # used later on, in a function, to print directory contents\n",
    "from os.path import exists, basename, join\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "from PIL.Image import fromarray\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directories for the Processed of datasets\n",
    "\n",
    "This section of the notebook will find a way to create directories for the images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file order of the dataset is important as we have manually segmented and manually tracked pictures, which we do not plan on processing. We need to find a way to generate the 2 processed datasets without altering this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial option to consider, is generating a list of all the file paths to our images..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite simple, thankfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(startPath):\n",
    "    location_array = []\n",
    "    acceptable_folders = [\"\\\\01\", \"\\\\02\", \"SEG\", \"TRA\"]\n",
    "\n",
    "    for root, dirs, files in walk(startPath):\n",
    "        # skip this folder\n",
    "        if (\"OriginalZipped\" in root):\n",
    "            continue\n",
    "\n",
    "        elif (root[ -3 : ] not in acceptable_folders):\n",
    "            continue\n",
    "\n",
    "        location_array.append(root)\n",
    "    \n",
    "    return location_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = getcwd()\n",
    "desired_directory = \"..\\\\..\\\\Comp700_Processed_DataSets_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (current_directory + \"\\\\\" + desired_directory)\n",
    "location_array = get_directories(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']\n",
      "Number of folders: 96\n"
     ]
    }
   ],
   "source": [
    "# first 10\n",
    "print( location_array[0:10] ) \n",
    "print(\"Number of folders:\", len( location_array ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can use that variable to generate the locations for our processed images! We just need to replace the keyword \"Comp700_DataSets\" with our desired folder name, and everything else will follow nicely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further improve the folder readability though, by only keeping the Comp700_DataSets etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_string_array(position, array):\n",
    "    new_array = []\n",
    "\n",
    "    for item in array:\n",
    "        new_array.append( item[position : ])\n",
    "    \n",
    "    return new_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = len(current_directory + \"\\\\..\\\\..\\\\\")\n",
    "# print(position)\n",
    "\n",
    "reduced_location_array = cut_string_array(position, location_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01', 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']\n",
      "\n",
      "Number of folders: 96\n"
     ]
    }
   ],
   "source": [
    "# first 10\n",
    "print(reduced_location_array[ 0 : 10])\n",
    "print()\n",
    "print(\"Number of folders:\", len( reduced_location_array ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the keyword now to our destination folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_part_of_array(key_word, new_word, array):\n",
    "    new_array = []\n",
    "    temp = \"\"\n",
    "\n",
    "    for item in array:\n",
    "        temp = item.replace(key_word, new_word)\n",
    "        new_array.append(temp)\n",
    "    \n",
    "    return new_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_locations = replace_part_of_array(\"Comp700_Processed_DataSets_1\", \"Comp700_Segmented\", reduced_location_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA', 'Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG', 'Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01', 'Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']\n",
      "Number of folders: 96\n"
     ]
    }
   ],
   "source": [
    "print( desired_locations[0:10] )\n",
    "print(\"Number of folders:\", len( desired_locations ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! We now have a variable containing the folder locations! We can now define some functions to validate all directories exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for work we create\n",
    "def tryMakeDirectory(current_directory, destination_directory):\n",
    "    try:\n",
    "        # join comes from os.path\n",
    "        mkdir( join(current_directory, destination_directory) )\n",
    "    except FileExistsError:\n",
    "        # print(\"Folder already exists!\")\n",
    "        pass\n",
    "    except:\n",
    "        print(\"Unknown Error Encountered...\")\n",
    "###\n",
    "\n",
    "def createBulkDirectories(current_directory, array):\n",
    "    sub_folders = []\n",
    "    path = \"..\\\\..\\\\\"\n",
    "\n",
    "    for item in array:\n",
    "        sub_folders = item.split(\"\\\\\")\n",
    "        # print(sub_folders)\n",
    "\n",
    "        for folder in sub_folders:\n",
    "            path += folder\n",
    "            tryMakeDirectory(current_directory, path)\n",
    "            path += \"\\\\\"\n",
    "        \n",
    "        # reset\n",
    "        path = \"..\\\\..\\\\\"\n",
    "\n",
    "    print(\"Done!\")\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "createBulkDirectories(current_directory, desired_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset Segmentation\n",
    "\n",
    "This section of the notebook focusses on processing the entire dataset, following the methods found in 005 for 1103_10 and 1103_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to take advantage of the Thresholding found with OpenCV - specifically the mask value of 17. Let's create a function to do that processing for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opencvThresh(img, value=17):\n",
    "    img = np.array(img).astype(np.uint8)\n",
    "    ret, thresh = cv2.threshold(img, 0, 255, value)\n",
    "\n",
    "    return thresh\n",
    "###\n",
    "\n",
    "# used to make the segmented values visible, by saving via matplotlib\n",
    "def getImage(filePath):\n",
    "    img = plt.imread(filePath) \n",
    "    plt.imsave(\"temp.jpg\", img, cmap=\"gray\") # desired colourmap for us\n",
    "    img = cv2.imread( \"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    return img\n",
    "###\n",
    "\n",
    "# process choice influences processOne or processTwo\n",
    "def bulkProcess(current_directory, original_dataset, location_array):\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    counter = 0\n",
    "    valid_folders = [\"01\", \"02\", \"SEG\", \"TRA\"]\n",
    "\n",
    "    name = \"segmented_\"\n",
    "\n",
    "    # go to the original_dataset\n",
    "    path = walk(current_directory + \"\\\\\" + original_dataset)\n",
    "    \n",
    "    print(\"Starting...\")\n",
    "\n",
    "    for root, dirs, files in path:\n",
    "        # skip zipped files\n",
    "        if (\"OriginalZipped\" in root):\n",
    "            continue\n",
    "        # end loop because locations exhausted\n",
    "        elif (counter >= len(location_array)):\n",
    "            break\n",
    "\n",
    "        # print(root)\n",
    "\n",
    "        for item in files:\n",
    "            # manual info, simply copy as is\n",
    "            if (\"man_\" in item):\n",
    "                # print(\"Counter:\", counter)\n",
    "                img_path = current_directory + \"\\\\..\\\\..\\\\\" + location_array[counter] + \"\\\\\" +  item\n",
    "                # print(img_path)\n",
    "\n",
    "                # handle text files\n",
    "                if (\".txt\" in item):\n",
    "                    copyfile(root + \"\\\\\" + item, img_path)\n",
    "                else:\n",
    "                    # print(\"EISH\")\n",
    "                    # img = getImage(root + \"\\\\\" + item)\n",
    "                    # should be able to read and save, as 006 did a good job with format before\n",
    "                    img = cv2.imread(root + \"\\\\\" + item) \n",
    "                    cv2.imwrite(img_path, img)\n",
    "                \n",
    "\n",
    "                \n",
    "            # stop working, zipped files found\n",
    "            elif (\".zip\" in item):\n",
    "                break\n",
    "            else:\n",
    "                # print(\"Nope\")\n",
    "\n",
    "                img = getImage(root + \"\\\\\" + item)\n",
    "                # pic_path = root + \"\\\\\" + item\n",
    "                # img = cv2.imread(pic_path)\n",
    "                # cv2.imshow(\"Pic\", img)\n",
    "                # cv2.waitKey(0)\n",
    "\n",
    "                processed_pic = opencvThresh(img)\n",
    "\n",
    "                # print(\"Counter:\", counter)\n",
    "                img_path = current_directory + \"\\\\..\\\\..\\\\\" + location_array[counter] + \"\\\\\" + name + item\n",
    "                # print(img_path)\n",
    "\n",
    "                cv2.imwrite(img_path, processed_pic)\n",
    "\n",
    "            # remove later\n",
    "            # break\n",
    "        \n",
    "        # update counter\n",
    "        if (basename(root) in valid_folders):\n",
    "            counter += 1\n",
    "    \n",
    "    # remove at end\n",
    "    if (exists(\"temp.jpg\")):\n",
    "        remove(\"temp.jpg\")\n",
    "    \n",
    "    print(\"Finished...\")\n",
    "### \n",
    "\n",
    "def getFileQuantities(path):\n",
    "    count = 0\n",
    "    size_array = []\n",
    "    valid_folders = [\"01\", \"02\", \"SEG\", \"TRA\"]\n",
    "\n",
    "    for root, dirs, files in walk(path):\n",
    "        count = 0\n",
    "\n",
    "        for file in files:\n",
    "            count += 1\n",
    "        \n",
    "        if (basename(root) in valid_folders):\n",
    "            size_array.append(count)\n",
    "    \n",
    "    return size_array\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sizes = getFileQuantities( current_directory + \"\\\\\" + \"..\\\\..\\\\Comp700_DataSets\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1764,\n",
       " 49,\n",
       " 1765,\n",
       " 1764,\n",
       " 1764,\n",
       " 8,\n",
       " 1765,\n",
       " 1764,\n",
       " 1763,\n",
       " 1763,\n",
       " 1375,\n",
       " 50,\n",
       " 1377,\n",
       " 1376,\n",
       " 1376,\n",
       " 50,\n",
       " 1377,\n",
       " 1376,\n",
       " 1376,\n",
       " 1375,\n",
       " 84,\n",
       " 9,\n",
       " 85,\n",
       " 84,\n",
       " 84,\n",
       " 9,\n",
       " 85,\n",
       " 84,\n",
       " 115,\n",
       " 115,\n",
       " 30,\n",
       " 8,\n",
       " 31,\n",
       " 30,\n",
       " 5,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 48,\n",
       " 18,\n",
       " 49,\n",
       " 48,\n",
       " 48,\n",
       " 33,\n",
       " 49,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 92,\n",
       " 30,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 20,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 150,\n",
       " 150,\n",
       " 151,\n",
       " 110,\n",
       " 138,\n",
       " 92,\n",
       " 28,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 8,\n",
       " 93,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 115,\n",
       " 15,\n",
       " 116,\n",
       " 115,\n",
       " 115,\n",
       " 19,\n",
       " 116,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 300,\n",
       " 2,\n",
       " 301,\n",
       " 300,\n",
       " 300,\n",
       " 2,\n",
       " 301,\n",
       " 300,\n",
       " 300,\n",
       " 300]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_sizes = getFileQuantities( current_directory + \"\\\\\" + \"..\\\\..\\\\Comp700_Segmented\" )\n",
    "\n",
    "segmented_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "Generating now\n",
      "Starting...\n",
      "Finished...\n"
     ]
    }
   ],
   "source": [
    "if (original_sizes == segmented_sizes):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")\n",
    "    print(\"\\nGenerating now\")\n",
    "    bulkProcess(current_directory, \"..\\\\..\\\\Comp700_Processed_DataSets_1\", desired_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some folders provided by the datasets that contain manually segmented images. Let us go through the segmented folder now and identify the quantities of images. We can then generate some videos and stitch them side by side, to identify the success of the segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01',\n",
       " 'Comp700_Processed_DataSets_1\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We can take advantage of reduced_location_array to map segmented images onto the corresponding folder\n",
    "'''\n",
    "\n",
    "reduced_location_array[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_location_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "manSegPicArray = [0 for i in range(len(reduced_location_array))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manSegPicArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We can take advantage of reduced_location_array to map segmented images onto the corresponding folder,\n",
    "inside manSegPicArray\n",
    "'''\n",
    "\n",
    "desired_pics = \"COMP700_Segmented\"\n",
    "\n",
    "path = walk(current_directory + \"\\\\..\\\\..\\\\\" + desired_pics)\n",
    "    \n",
    "keyword = \"man_seg\"\n",
    "count = 0\n",
    "\n",
    "for root, dirs, files in path:\n",
    "    # print(files)\n",
    "    # print(count, end=\"; \")\n",
    "    for item in files:\n",
    "        if (keyword in item):\n",
    "            # print(len(files))\n",
    "            manSegPicArray[count] = len(files)\n",
    "            break\n",
    "        break\n",
    "\n",
    "    # only update if non empty set!\n",
    "    if ( len(files) != 0):\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 49,\n",
       " 0,\n",
       " 1764,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 1764,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 50,\n",
       " 0,\n",
       " 1376,\n",
       " 0,\n",
       " 50,\n",
       " 0,\n",
       " 1376,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 84,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 84,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 48,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 48,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 30,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 65,\n",
       " 0,\n",
       " 0,\n",
       " 150,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 28,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 115,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 115,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 300,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 300,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manSegPicArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's marry this information together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_GT\\SEG ... 49\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_ST\\SEG ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_GT\\SEG ... 8\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_ST\\SEG ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_GT\\SEG ... 50\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_ST\\SEG ... 1376\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_GT\\SEG ... 50\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_ST\\SEG ... 1376\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_GT\\SEG ... 9\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_GT\\SEG ... 9\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG ... 84\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\01_GT\\SEG ... 8\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\02_GT\\SEG ... 5\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_GT\\SEG ... 18\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_ST\\SEG ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_GT\\SEG ... 33\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_ST\\SEG ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_GT\\SEG ... 30\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_GT\\SEG ... 20\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\01_GT\\SEG ... 65\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\02_GT\\SEG ... 150\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_GT\\SEG ... 28\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_GT\\SEG ... 8\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_ST\\SEG ... 92\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_GT\\SEG ... 15\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_ST\\SEG ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_GT\\SEG ... 19\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_ST\\SEG ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_GT\\SEG ... 2\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_ST\\SEG ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_GT\\SEG ... 2\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_ST\\SEG ... 300\n",
      "\n",
      "There are 36 folders of segmented images\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(manSegPicArray)):\n",
    "    if (manSegPicArray[i] != 0):\n",
    "        print(reduced_location_array[i], manSegPicArray[i], sep=\" ... \")\n",
    "        count += 1\n",
    "\n",
    "print(\"\\nThere are\", count, \"folders of segmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extend that summary to see how many images are in the corresponding folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondingPicArray = [0 for i in range(len(reduced_location_array))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_pics = \"COMP700_Segmented\"\n",
    "\n",
    "path = walk(current_directory + \"\\\\..\\\\..\\\\\" + desired_pics)\n",
    "    \n",
    "keyword1 = \"_GT\"; keyword2 = \"_ST\"\n",
    "count = 0\n",
    "index = 0\n",
    "\n",
    "for root, dirs, files in path:\n",
    "    # print(files)\n",
    "    # print(count, end=\"; \")\n",
    "    for item in files:\n",
    "        if (keyword1 in reduced_location_array[index]) or (keyword2 in reduced_location_array[index]):\n",
    "            # print(len(files))\n",
    "            # print(count)\n",
    "            correspondingPicArray[index] = count\n",
    "            break\n",
    "        else:\n",
    "            count = len(files)\n",
    "            break\n",
    "    \n",
    "    if (len(files) != 0):\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1764,\n",
       " 1764,\n",
       " 1764,\n",
       " 0,\n",
       " 1764,\n",
       " 1764,\n",
       " 1764,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1375,\n",
       " 1375,\n",
       " 1375,\n",
       " 0,\n",
       " 1376,\n",
       " 1376,\n",
       " 1376,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 0,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 30,\n",
       " 30,\n",
       " 0,\n",
       " 30,\n",
       " 30,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 0,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 65,\n",
       " 65,\n",
       " 0,\n",
       " 150,\n",
       " 150,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 0,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 300,\n",
       " 300,\n",
       " 300,\n",
       " 0,\n",
       " 300,\n",
       " 300,\n",
       " 300,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correspondingPicArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_GT\\SEG ... Segmented Pictures: ... 49 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\01_ST\\SEG ... Segmented Pictures: ... 1764 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_GT\\SEG ... Segmented Pictures: ... 8 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-HSC\\BF-C2DL-HSC\\02_ST\\SEG ... Segmented Pictures: ... 1764 ... Corresponding Pictures: ... 1764\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_GT\\SEG ... Segmented Pictures: ... 50 ... Corresponding Pictures: ... 1375\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\01_ST\\SEG ... Segmented Pictures: ... 1376 ... Corresponding Pictures: ... 1375\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_GT\\SEG ... Segmented Pictures: ... 50 ... Corresponding Pictures: ... 1376\n",
      "Comp700_Processed_DataSets_1\\BF-C2DL-MuSC\\BF-C2DL-MuSC\\02_ST\\SEG ... Segmented Pictures: ... 1376 ... Corresponding Pictures: ... 1376\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_GT\\SEG ... Segmented Pictures: ... 9 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\01_ST\\SEG ... Segmented Pictures: ... 84 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_GT\\SEG ... Segmented Pictures: ... 9 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\DIC-C2DH-HeLa\\DIC-C2DH-HeLa\\02_ST\\SEG ... Segmented Pictures: ... 84 ... Corresponding Pictures: ... 84\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\01_GT\\SEG ... Segmented Pictures: ... 8 ... Corresponding Pictures: ... 30\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-Huh7\\Fluo-C2DL-Huh7\\02_GT\\SEG ... Segmented Pictures: ... 5 ... Corresponding Pictures: ... 30\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_GT\\SEG ... Segmented Pictures: ... 18 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\01_ST\\SEG ... Segmented Pictures: ... 48 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_GT\\SEG ... Segmented Pictures: ... 33 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-C2DL-MSC\\Fluo-C2DL-MSC\\02_ST\\SEG ... Segmented Pictures: ... 48 ... Corresponding Pictures: ... 48\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_GT\\SEG ... Segmented Pictures: ... 30 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\01_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_GT\\SEG ... Segmented Pictures: ... 20 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-GOWT1\\Fluo-N2DH-GOWT1\\02_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\01_GT\\SEG ... Segmented Pictures: ... 65 ... Corresponding Pictures: ... 65\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DH-SIM+\\Fluo-N2DH-SIM+\\02_GT\\SEG ... Segmented Pictures: ... 150 ... Corresponding Pictures: ... 150\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_GT\\SEG ... Segmented Pictures: ... 28 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\01_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_GT\\SEG ... Segmented Pictures: ... 8 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\Fluo-N2DL-HeLa\\Fluo-N2DL-HeLa\\02_ST\\SEG ... Segmented Pictures: ... 92 ... Corresponding Pictures: ... 92\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_GT\\SEG ... Segmented Pictures: ... 15 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\01_ST\\SEG ... Segmented Pictures: ... 115 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_GT\\SEG ... Segmented Pictures: ... 19 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DH-U373\\PhC-C2DH-U373\\02_ST\\SEG ... Segmented Pictures: ... 115 ... Corresponding Pictures: ... 115\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_GT\\SEG ... Segmented Pictures: ... 2 ... Corresponding Pictures: ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\01_ST\\SEG ... Segmented Pictures: ... 300 ... Corresponding Pictures: ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_GT\\SEG ... Segmented Pictures: ... 2 ... Corresponding Pictures: ... 300\n",
      "Comp700_Processed_DataSets_1\\PhC-C2DL-PSC\\PhC-C2DL-PSC\\02_ST\\SEG ... Segmented Pictures: ... 300 ... Corresponding Pictures: ... 300\n",
      "\n",
      "There are 36 folders of segmented images\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i in range(len(manSegPicArray)):\n",
    "    if (manSegPicArray[i] != 0):\n",
    "        print(reduced_location_array[i], \"Segmented Pictures:\", manSegPicArray[i], \"Corresponding Pictures:\", correspondingPicArray[i], sep=\" ... \")\n",
    "        count += 1\n",
    "\n",
    "print(\"\\nThere are\", count, \"folders of segmented images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed look at those results indicates that the GT Segmentation always <= quantity of images in the corresponding folder. Perhaps GT stands for Generated Trace? ST, on the other hand, is always = quantity of images in the corresponding folder.\n",
    "\n",
    "FLUO-N2DL-SIM+ as well as Fluo-C2DL-Huh7 also contain 2 folders of segmentated images, all the rest have 4. Meaning 8 * 4 + 2 * 2 = 32 + 4 = 36\n",
    "\n",
    "We can use this information to generate 10 videos, each stitched together, and watch them to see the the segmentation differences. We can then decide if we need to further segment, or can move to post processing immediately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import move # moves and replaces files\n",
    "\n",
    "from moviepy.editor import clips_array, VideoFileClip\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need an array of the folder locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Array\n",
      "['BF-C2DL-HSC', 'BF-C2DL-HSC (1)', 'BF-C2DL-MuSC', 'BF-C2DL-MuSC (1)', 'DIC-C2DH-HeLa', 'DIC-C2DH-HeLa (1)', 'Fluo-C2DL-Huh7', 'Fluo-C2DL-Huh7 (1)', 'Fluo-C2DL-MSC', 'Fluo-C2DL-MSC (1)', 'Fluo-N2DH-GOWT1', 'Fluo-N2DH-GOWT1 (1)', 'Fluo-N2DH-SIM+', 'Fluo-N2DH-SIM+ (1)', 'Fluo-N2DL-HeLa', 'Fluo-N2DL-HeLa (1)', 'PhC-C2DH-U373', 'PhC-C2DH-U373 (1)', 'PhC-C2DL-PSC', 'PhC-C2DL-PSC (1)']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We only need to show every _OTHER_ folder, as each dataset has a \n",
    " training and challenge set. So out of 20 files, we need to show 10\n",
    "\n",
    "First things first, let us create an array of the directory locations\n",
    "'''\n",
    "\n",
    "data_sets = \"..\\\\..\\\\Comp700_DataSets\"\n",
    "current_directory = getcwd()\n",
    "\n",
    "path = walk(current_directory + \"\\\\\" + data_sets)\n",
    "\n",
    "directory_array = [] # contains the main folders\n",
    "\n",
    "i = 1\n",
    "for root, dirs, files in path:\n",
    "    if (i == 2):\n",
    "        directory_array = dirs\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"Directory Array\")\n",
    "print(directory_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_GT\\\\TRA\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\01_ST\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_GT\\\\TRA\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC\\\\BF-C2DL-HSC\\\\02_ST\\\\SEG\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\01\\\\', 'c:\\\\Users\\\\G5\\\\Documents\\\\GitHub\\\\COMP700\\\\..\\\\..\\\\Comp700_Segmented\\\\BF-C2DL-HSC (1)\\\\BF-C2DL-HSC (1)\\\\02\\\\']\n",
      "\n",
      " 96\n"
     ]
    }
   ],
   "source": [
    "def replace_part_of_array(key_word, new_word, array):\n",
    "    new_array = []\n",
    "    temp = \"\"\n",
    "\n",
    "    for item in array:\n",
    "        temp = item.replace(key_word, new_word)\n",
    "        new_array.append(temp)\n",
    "    \n",
    "    return new_array\n",
    "###\n",
    "\n",
    "# First, generate a list of the locations for each folder of Petri Dish images\n",
    "data_sets = \"..\\\\..\\\\Comp700_Segmented\"\n",
    "path = walk(current_directory + \"\\\\\" + data_sets) # reset path\n",
    "\n",
    "location_array = reduced_location_array\n",
    "\n",
    "location_array = replace_part_of_array(\"Comp700_Processed_DataSets_1\", \"Comp700_Segmented\", location_array)\n",
    "\n",
    "for i in range(len(location_array)):\n",
    "    location_array[i] = current_directory + \"\\\\..\\\\..\\\\\" + location_array[i] + \"\\\\\"\n",
    "\n",
    "\n",
    "print(location_array[0:10])\n",
    "\n",
    "print(\"\\n\", len(location_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BF-C2DL-HSC_01', 'BF-C2DL-HSC_01_GT_SEG', 'BF-C2DL-HSC_01_GT_TRA', 'BF-C2DL-HSC_01_ST_SEG', 'BF-C2DL-HSC_02', 'BF-C2DL-HSC_02_GT_SEG', 'BF-C2DL-HSC_02_GT_TRA', 'BF-C2DL-HSC_02_ST_SEG', 'BF-C2DL-HSC (1)_01', 'BF-C2DL-HSC (1)_02']\n"
     ]
    }
   ],
   "source": [
    "# generate this for the videos!\n",
    "folderNameArray = []\n",
    "\n",
    "for i in range(len(location_array)):\n",
    "    temp = location_array[i].replace(current_directory + \"\\\\..\\\\..\\\\\" + \"Comp700_Segmented\\\\\", \"\")\n",
    "    \n",
    "\n",
    "    #remove leading folder name and trailing braces\n",
    "    position = temp.index(\"\\\\\")\n",
    "    temp = temp[ position + 1 : -1 ]\n",
    "\n",
    "    temp = temp.replace(\"\\\\\", \"_\")\n",
    "\n",
    "    folderNameArray.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "print(folderNameArray[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know from 003 that the images have consistent dimensions. So, we fetch the dimensions using the first image from each folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1010, 1010], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [1036, 1070], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [512, 512], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [832, 992], [832, 992], [832, 992], [832, 992], [782, 1200], [782, 1200], [782, 1200], [782, 1200], [832, 992], [782, 1200], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [1024, 1024], [690, 628], [690, 628], [690, 628], [773, 739], [773, 739], [773, 739], [718, 660], [790, 664], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [700, 1100], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [520, 696], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720], [576, 720]]\n"
     ]
    }
   ],
   "source": [
    "image_size_array = []\n",
    "\n",
    "path = walk(current_directory + \"\\\\\" + data_sets) # reset path\n",
    "\n",
    "i = -1 # will grow from 0 to 39\n",
    "for root, dirs, files in path:\n",
    "    for item in files:\n",
    "        # print(item)\n",
    "        if (\".txt\" not in item) and (\".zip\" not in item):\n",
    "            i += 1\n",
    "\n",
    "            img = cv2.imread( (location_array[i] + item), cv2.IMREAD_GRAYSCALE)\n",
    "            (x, y) = img.shape\n",
    "\n",
    "            image_size_array.append([x, y])\n",
    "            break\n",
    "            \n",
    "\n",
    "print(image_size_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateVideos(current_directory, desired_folder, use_colour):\n",
    "    # only progress if files don't exist\n",
    "    makeVideos = False\n",
    "\n",
    "    if (exists(current_directory + \"\\\\\" + desired_folder)):\n",
    "        # Now, go to directory and verify all is there\n",
    "        path = walk(current_directory + \"\\\\\" + desired_folder)\n",
    "\n",
    "        count = 0\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                count += 1\n",
    "\n",
    "        if (count == len(location_array)):\n",
    "            print(\"All Videos exist already!\")\n",
    "        else:\n",
    "            print(\"Not all Videos exist\")\n",
    "            makeVideos = True\n",
    "    else:\n",
    "        makeVideos = True\n",
    "        \n",
    "    if (makeVideos):\n",
    "        path = walk(current_directory + \"\\\\\" + data_sets) # reset path\n",
    "\n",
    "        i = -1\n",
    "        output_video = cv2.VideoWriter()\n",
    "        frames_per_second = 10\n",
    "        picNum = 0\n",
    "        fileName = \"\"\n",
    "\n",
    "        # Generates Colour Videos\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                if (\".txt\" not in item) and (\".zip\" not in item):\n",
    "                    picNum += 1\n",
    "                    # update on first element only\n",
    "                    if  (picNum == 1):\n",
    "                        i += 1\n",
    "                        index = i // 2 # used for output video as 2 copies for each directory\n",
    "\n",
    "                        size = (image_size_array[i][1], image_size_array[i][0] ) # notice order\n",
    "                        fileName = \"video_segmented_\" + folderNameArray[i] + \".mp4\"\n",
    "                        \n",
    "                        output_video = cv2.VideoWriter(\n",
    "                            fileName, \n",
    "                            cv2.VideoWriter_fourcc(*'DIVX'), \n",
    "                            frames_per_second, \n",
    "                            size,\n",
    "                            isColor=use_colour # either True or False\n",
    "                        )\n",
    "\n",
    "                    img = plt.imread(location_array[i] + item) \n",
    "                    plt.imsave(\"temp.jpg\", img, cmap='gray')\n",
    "                    img = cv2.imread( \"temp.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "                      \n",
    "                    output_video.write(img)\n",
    "\n",
    "            picNum = 0 # reset\n",
    "\n",
    "            if (len(fileName) != 0):\n",
    "                cv2.destroyAllWindows()\n",
    "                output_video.release()\n",
    "                print(\"Video finished for \", fileName, sep=\"\")\n",
    "    \n",
    "    # remove at end\n",
    "    if (exists(\"temp.jpg\")):\n",
    "        remove(\"temp.jpg\")\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os.path import join\n",
    "# from shutil import move # moves and replaces files\n",
    "\n",
    "def moveBulkVideos(current_directory, desired_folder):\n",
    "    # only progress if files don't exist\n",
    "    if (exists(current_directory + \"\\\\\" + desired_folder)):\n",
    "        print(\"Videos already exist!\")\n",
    "    else:\n",
    "        # local function\n",
    "        tryMakeDirectory(current_directory, desired_folder)\n",
    "\n",
    "        path = walk(current_directory)\n",
    "\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                if (\".mp4\" in item):\n",
    "                    new_destination = current_directory + \"\\\\\" + desired_folder\n",
    "                    move(join(current_directory, item), join(new_destination, item)) # should overwrite existing data\n",
    "\n",
    "        # Now, go to directory and verify all is there\n",
    "        path = walk(current_directory + \"\\\\\" + desired_folder)\n",
    "\n",
    "        count = 0\n",
    "        for root, dirs, files in path:\n",
    "            for item in files:\n",
    "                count += 1\n",
    "\n",
    "        if (count == len(location_array)):\n",
    "            print(\"All Videos Moved Successfully!\")\n",
    "        else:\n",
    "            print(\"Not all Videos Moves Successfully\")\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Videos exist already!\n"
     ]
    }
   ],
   "source": [
    "generateVideos(current_directory, \"..\\\\..\\\\Comp700_VideosOfSegmentation\", use_colour=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos already exist!\n"
     ]
    }
   ],
   "source": [
    "moveBulkVideos(current_directory, \"..\\\\..\\\\Comp700_VideosOfSegmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Comaprisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can do now is to compare the collection of segmentation videos together. We can then compare videos against the provided Solutions and see how we are doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the videos from the sama dataset together now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need the locations and names of the videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_segmented_BF-C2DL-HSC (1)_01.mp4', 'video_segmented_BF-C2DL-HSC (1)_02.mp4', 'video_segmented_BF-C2DL-HSC_01.mp4', 'video_segmented_BF-C2DL-HSC_01_GT_SEG.mp4', 'video_segmented_BF-C2DL-HSC_01_GT_TRA.mp4', 'video_segmented_BF-C2DL-HSC_01_ST_SEG.mp4', 'video_segmented_BF-C2DL-HSC_02.mp4', 'video_segmented_BF-C2DL-HSC_02_GT_SEG.mp4', 'video_segmented_BF-C2DL-HSC_02_GT_TRA.mp4', 'video_segmented_BF-C2DL-HSC_02_ST_SEG.mp4']\n",
      "\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "desired_directory = getcwd() + \"\\\\..\\\\..\\\\\" + \"Comp700_VideosOfSegmentation\"\n",
    "\n",
    "path = walk(desired_directory)\n",
    "\n",
    "videoFiles = []\n",
    "\n",
    "for root, dirs, files in path:\n",
    "    videoFiles = files\n",
    "    break\n",
    "\n",
    "print(videoFiles[0:10])\n",
    "print()\n",
    "print(len(videoFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the above that the best way to find out which files belong together (automatically) is to remove the last 3 characters and check for String equality. I can also generate a label array of the 10 datasets to use later\n",
    "\n",
    "Lets do the label array first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BF-C2DL-HSC', 'BF-C2DL-HSC (1)', 'BF-C2DL-MuSC', 'BF-C2DL-MuSC (1)', 'DIC-C2DH-HeLa', 'DIC-C2DH-HeLa (1)', 'Fluo-C2DL-Huh7', 'Fluo-C2DL-Huh7 (1)', 'Fluo-C2DL-MSC', 'Fluo-C2DL-MSC (1)', 'Fluo-N2DH-GOWT1', 'Fluo-N2DH-GOWT1 (1)', 'Fluo-N2DH-SIM+', 'Fluo-N2DH-SIM+ (1)', 'Fluo-N2DL-HeLa', 'Fluo-N2DL-HeLa (1)', 'PhC-C2DH-U373', 'PhC-C2DH-U373 (1)', 'PhC-C2DL-PSC', 'PhC-C2DL-PSC (1)']\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "desired_directory = getcwd() + \"\\\\..\\\\..\\\\\" + \"Comp700_DataSets\\\\Extracted\"\n",
    "\n",
    "path = walk(desired_directory)\n",
    "\n",
    "labels = []\n",
    "\n",
    "temp = []\n",
    "\n",
    "for root, dirs, files in path:\n",
    "    labels = dirs\n",
    "    break\n",
    "\n",
    "print(labels)\n",
    "print()\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to swap every neighbour, so that the labels with parenthesis appear first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BF-C2DL-HSC (1)', 'BF-C2DL-HSC', 'BF-C2DL-MuSC (1)', 'BF-C2DL-MuSC', 'DIC-C2DH-HeLa (1)', 'DIC-C2DH-HeLa', 'Fluo-C2DL-Huh7 (1)', 'Fluo-C2DL-Huh7', 'Fluo-C2DL-MSC (1)', 'Fluo-C2DL-MSC', 'Fluo-N2DH-GOWT1 (1)', 'Fluo-N2DH-GOWT1', 'Fluo-N2DH-SIM+ (1)', 'Fluo-N2DH-SIM+', 'Fluo-N2DL-HeLa (1)', 'Fluo-N2DL-HeLa', 'PhC-C2DH-U373 (1)', 'PhC-C2DH-U373', 'PhC-C2DL-PSC (1)', 'PhC-C2DL-PSC']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temp = labels.copy()\n",
    "newLabels = temp.copy()\n",
    "\n",
    "for i in range(len(temp)):\n",
    "    if (i == len(labels)-1):\n",
    "        newLabels[i] = temp[i-1]\n",
    "    elif (i % 2 == 1):\n",
    "        newLabels[i] = temp[i-1]\n",
    "    else:\n",
    "        newLabels[i] = temp[i+1]\n",
    "\n",
    "print(newLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We can now identify which videos should be clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 2, 8, 2, 8, 2, 6, 2, 8, 2, 8, 2, 6, 2, 8, 2, 8, 2, 8]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to traverse the videoFiles, looking for the \n",
    "#  keywords found in newLabels\n",
    "\n",
    "countArray = [0 for i in range(len(videoFiles))]\n",
    "prevKeyword = \"\"; temp = \"\"\n",
    "keywordIndex = -1\n",
    "\n",
    "for i in range(len(videoFiles)):\n",
    "    temp = videoFiles[i]\n",
    "\n",
    "    for j in range(len(newLabels)):\n",
    "        if (newLabels[j] in temp):\n",
    "            keywordIndex = j\n",
    "            break\n",
    "    \n",
    "    countArray[i] = keywordIndex\n",
    "\n",
    "# simple counting algorithm - counts quantity of numbers, because sorted list\n",
    "start = -1; end = -1; count = -1\n",
    "quantityArray = []\n",
    "\n",
    "for k in range(len(videoFiles)):\n",
    "    if (k == 0):\n",
    "        start = k\n",
    "        count = countArray[k]\n",
    "        continue\n",
    "\n",
    "    if (countArray[k] != count):\n",
    "        end = k\n",
    "        count = countArray[k]\n",
    "        quantityArray.append(end-start)\n",
    "        start = k\n",
    "# update at end as well\n",
    "quantityArray.append( (k+1)-start)\n",
    "\n",
    "quantityArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significance of that variable, quantityArray, is that we can use it to identity how many videos need to be stitched together.\n",
    "\n",
    "We will then stitch the videos together, and watch the videos to identify trends in the dataset, after segmentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the 2 variables below to find the locations of the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G5\\Documents\\GitHub\\COMP700\\..\\..\\Comp700_VideosOfSegmentation\n",
      "['video_segmented_BF-C2DL-HSC (1)_01.mp4', 'video_segmented_BF-C2DL-HSC (1)_02.mp4', 'video_segmented_BF-C2DL-HSC_01.mp4', 'video_segmented_BF-C2DL-HSC_01_GT_SEG.mp4', 'video_segmented_BF-C2DL-HSC_01_GT_TRA.mp4', 'video_segmented_BF-C2DL-HSC_01_ST_SEG.mp4', 'video_segmented_BF-C2DL-HSC_02.mp4', 'video_segmented_BF-C2DL-HSC_02_GT_SEG.mp4', 'video_segmented_BF-C2DL-HSC_02_GT_TRA.mp4', 'video_segmented_BF-C2DL-HSC_02_ST_SEG.mp4']\n"
     ]
    }
   ],
   "source": [
    "videoDestinations = getcwd() + \"\\\\..\\\\..\\\\\" + \"Comp700_VideosOfSegmentation\"\n",
    "print(videoDestinations)\n",
    "\n",
    "print(videoFiles[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we either need to stitch 2 videos together, 6 videos together or 8 videos together.\n",
    "\n",
    "We can use 'quantityArray' to automate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulkStitchVideos(test_directory):\n",
    "    name_array =    [\"vid1.mp4\", \"vid2.mp4\", \"vid3.mp4\", \"vid4.mp4\", \n",
    "                 \"vid5.mp4\", \"vid6.mp4\", \"vid7.mp4\", \"vid8.mp4\"]\n",
    "    videoIndex = -1; count = 0\n",
    "\n",
    "    # use quantityArray to stitch the videos together\n",
    "    for a in quantityArray:\n",
    "        \n",
    "        # print(a)\n",
    "        for b in range(a):\n",
    "            videoIndex += 1\n",
    "            c = VideoFileClip(videoDestinations + \"\\\\\" + videoFiles[videoIndex])\n",
    "\n",
    "            # getting only first 5 seconds\n",
    "            clip = c.subclip(0, 5)\n",
    "\n",
    "            # new clip with new duration\n",
    "            new_clip = clip.set_duration(10)\n",
    "\n",
    "            # reduce by 75%\n",
    "            resized_clip = new_clip.resize(0.25)\n",
    "\n",
    "            resized_clip.write_videofile(test_directory + \"\\\\\" + name_array[b])\n",
    "\n",
    "            # new_clip.ipython_display(width=100)\n",
    "        \n",
    "        if (a == 2):\n",
    "            a = VideoFileClip(test_directory + \"\\\\\" + name_array[0])\n",
    "            b = VideoFileClip(test_directory + \"\\\\\" + name_array[1])\n",
    "\n",
    "            # now, stitch together!\n",
    "            stitched_video = clips_array([[a, b]])\n",
    "        elif (a == 6):\n",
    "            a = VideoFileClip(test_directory + \"\\\\\" + name_array[0])\n",
    "            b = VideoFileClip(test_directory + \"\\\\\" + name_array[1])\n",
    "            c = VideoFileClip(test_directory + \"\\\\\" + name_array[2])\n",
    "            d = VideoFileClip(test_directory + \"\\\\\" + name_array[3])\n",
    "            e = VideoFileClip(test_directory + \"\\\\\" + name_array[4])\n",
    "            f = VideoFileClip(test_directory + \"\\\\\" + name_array[5])\n",
    "\n",
    "            # now, stitch together!\n",
    "            stitched_video = clips_array([[a, b, c], [d, e, f]])\n",
    "        elif (a == 8):\n",
    "            a = VideoFileClip(test_directory + \"\\\\\" + name_array[0])\n",
    "            b = VideoFileClip(test_directory + \"\\\\\" + name_array[1])\n",
    "            c = VideoFileClip(test_directory + \"\\\\\" + name_array[2])\n",
    "            d = VideoFileClip(test_directory + \"\\\\\" + name_array[3])\n",
    "            e = VideoFileClip(test_directory + \"\\\\\" + name_array[4])\n",
    "            f = VideoFileClip(test_directory + \"\\\\\" + name_array[5])\n",
    "            g = VideoFileClip(test_directory + \"\\\\\" + name_array[6])\n",
    "            h = VideoFileClip(test_directory + \"\\\\\" + name_array[7])\n",
    "\n",
    "            # now, stitch together!\n",
    "            stitched_video = clips_array([[a, b, c, d], [e, f, g, h]])\n",
    "        else:\n",
    "            print(\"Invalid number of videos specified\")\n",
    "            break\n",
    "\n",
    "        stitched_video.write_videofile(test_directory + \"\\\\\" + newLabels[count] + \".mp4\")\n",
    "        \n",
    "        resized_clip.close()\n",
    "        try:\n",
    "            a.close(); b.close(); c.close(); d.close()\n",
    "            e.close(); f.close(); g.close(); h.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        stitched_video.close()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # at end, remove videos:\n",
    "    for name in name_array:\n",
    "        if (exists(test_directory + \"\\\\\" + name)):\n",
    "            remove(test_directory + \"\\\\\" + name)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos Already created!\n"
     ]
    }
   ],
   "source": [
    "test_directory = \"009_Segmentation_Videos\"\n",
    "\n",
    "tryMakeDirectory(getcwd(), test_directory)\n",
    "\n",
    "path = walk(test_directory)\n",
    "\n",
    "for root, dirs, files in path:\n",
    "    fileCollection = files\n",
    "    break\n",
    "\n",
    "if ( not ( len(fileCollection) == 20 ) ):\n",
    "    bulkStitchVideos(test_directory)\n",
    "else:\n",
    "    print(\"Videos Already created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 20 videos generated, we can see a trend:\n",
    "\n",
    "The videos with a (1) attached are challenge sets - which contain training sets and no test sets\n",
    "\n",
    "The other videos contain 6 or 8 folders - 2 training sets, 2 or 4 test sets for segmentation and 2 test sets for tracking\n",
    "\n",
    "|      | Challenge Sets                                                      |   |                                  6/8 Sets                                    | \n",
    "|------|---------------------------------------------------------------------|---|------------------------------------------------------------------------------|\n",
    "| A(1) | Left video has some some pulsating frames, Right video mainly white | A | Top-Left video has some pulsating frames                                     |\n",
    "| B(1) | Left video has some some pulsating frames                           | B | Top-Left video has some pulsating frames, Far left videos are a bit 'soft'   |\n",
    "| C(1) | Both Left and Right video has some pulsating frames                 | C | Far left videos are a bit 'soft'                                             |\n",
    "| D(1) | Right video has some Dark Spots                                     | D |                                                                              |\n",
    "| E(1) |                                                                     | E |                                                                              |\n",
    "| F(1) |                                                                     | F |                                                                              |\n",
    "| G(1) |                                                                     | G |                                                                              |\n",
    "| H(1) |                                                                     | H |                                                                              |\n",
    "| I(1) |                                                                     | I |                                                                              |\n",
    "| J(1) | Both videos have some harsh light                                   | J | Far left videos have some harsh light                                        |\n",
    "\n",
    "The other videos are reasonably well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can try next is to implement an idea explored in the following video: https://www.youtube.com/watch?v=jvZm8REF2KY\n",
    "\n",
    "The author uses masks to train a Neural Network, using U-net Architecture.\n",
    "\n",
    "We can attempt to generate our own masks, by scanning the Test Images and looking for patterns! This may not work, if the colours are assigned randomly, but it is worth a try. We will explore this in another notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('python_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b82f63144f2c45b2313238722d81a7c4b83bac7fe75f18d324f42bf6a6cba197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
